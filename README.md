# Natural-language-processing-A-view-from-scratch
- ## daily paper reading
    - Attention all you need
    - GPT-2:Language Models areunsupervised multitask Learners 
    - BERT: Pre training of deep Bidirectional Transformer for Language understanding
    - RoBERTa: A Robustly optimized BERT pretraining Approach
    - REFORMER: The efficient transformer
    - Batch Normalization: Accelerating deep network training by reducing internal covariance shift